{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    source_id: str\n",
        "    source_type: str   # pdf | wikipedia | text\n",
        "    title: str\n",
        "    content: str\n",
        "    metadata: Dict\n",
        "\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    chunk_id: str\n",
        "    source_id: str\n",
        "    source_type: str\n",
        "    title: str\n",
        "    content: str\n",
        "    metadata: Dict\n",
        "\n",
        "@dataclass\n",
        "class WebSearchResult:\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "\n",
        "@dataclass\n",
        "class AnswerSource:\n",
        "    source_type: str   # Doc | Web\n",
        "    reference: str\n"
      ],
      "metadata": {
        "id": "bqB4HZ88SL9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-community\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, WikipediaLoader\n",
        "import uuid\n",
        "\n",
        "# Document class is defined in cell bqB4HZ88SL9L and will be available after execution\n",
        "\n",
        "def load_pdfs(pdf_paths):\n",
        "    docs = []\n",
        "    for path in pdf_paths:\n",
        "        loader = PyPDFLoader(path)\n",
        "        pages = loader.load()\n",
        "        full_text = \" \".join([p.page_content for p in pages])\n",
        "\n",
        "        docs.append(Document(\n",
        "            source_id=str(uuid.uuid4()),\n",
        "            source_type=\"pdf\",\n",
        "            title=path.split(\"/\")[-1],\n",
        "            content=full_text,\n",
        "            metadata={\"path\": path}\n",
        "        ))\n",
        "    return docs\n",
        "\n",
        "def load_text_files(paths):\n",
        "    docs = []\n",
        "    for path in paths:\n",
        "        loader = TextLoader(path)\n",
        "        data = loader.load()[0]\n",
        "\n",
        "        docs.append(Document(\n",
        "            source_id=str(uuid.uuid4()),\n",
        "            source_type=\"text\",\n",
        "            title=path.split(\"/\")[-1],\n",
        "            content=data.page_content,\n",
        "            metadata={\"path\": path}\n",
        "        ))\n",
        "    return docs\n",
        "\n",
        "def load_wikipedia(pages):\n",
        "    docs = []\n",
        "    for page in pages:\n",
        "        loader = WikipediaLoader(query=page, load_max_docs=1)\n",
        "        data = loader.load()[0]\n",
        "\n",
        "        docs.append(Document(\n",
        "            source_id=str(uuid.uuid4()),\n",
        "            source_type=\"wikipedia\",\n",
        "            title=page,\n",
        "            content=data.page_content,\n",
        "            metadata={\"url\": data.metadata.get(\"source\")}\n",
        "        ))\n",
        "    return docs"
      ],
      "metadata": {
        "id": "Yl59UlljSO5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "EXik7X9ZSe7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-text-splitters\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any\n",
        "import uuid\n",
        "\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    chunk_id: str\n",
        "    source_id: str\n",
        "    source_type: str\n",
        "    title: str\n",
        "    content: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "def chunk_documents(documents):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP\n",
        "    )\n",
        "\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        split_texts = splitter.split_text(doc.content)\n",
        "\n",
        "        for i, chunk in enumerate(split_texts):\n",
        "            chunks.append(DocumentChunk(\n",
        "                chunk_id=str(uuid.uuid4()),\n",
        "                source_id=doc.source_id,\n",
        "                source_type=doc.source_type,\n",
        "                title=doc.title,\n",
        "                content=chunk,\n",
        "                metadata={\"chunk_index\": i}\n",
        "            ))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "Cn6kNyirSwZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-openai langchain-core\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document as LCDocument\n",
        "import os\n",
        "\n",
        "def index_documents(chunks, index_path=\"faiss_index\"):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    lc_docs = [\n",
        "        LCDocument(\n",
        "            page_content=c.content,\n",
        "            metadata={\n",
        "                \"title\": c.title,\n",
        "                \"source_type\": c.source_type,\n",
        "                \"chunk_index\": c.metadata[\"chunk_index\"]\n",
        "            }\n",
        "        )\n",
        "        for c in chunks\n",
        "    ]\n",
        "\n",
        "    db = FAISS.from_documents(lc_docs, embeddings)\n",
        "    db.save_local(index_path)\n",
        "\n",
        "def load_faiss_index(index_path=\"faiss_index\"):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "S7V9QkdUSztO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(db, query, k=5):\n",
        "    return db.similarity_search(query, k=k)\n"
      ],
      "metadata": {
        "id": "IcFa6D62TFQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_query(query: str):\n",
        "    query = query.lower()\n",
        "\n",
        "    if any(word in query for word in [\"latest\", \"recent\", \"current\", \"today\", \"news\"]):\n",
        "        return \"web\"\n",
        "\n",
        "    if any(word in query for word in [\"compare\", \"difference\", \"vs\"]):\n",
        "        return \"hybrid\"\n",
        "\n",
        "    return \"document\"\n"
      ],
      "metadata": {
        "id": "Bs9xWNyiTeX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "def tavily_search(query, k=5):\n",
        "    tool = TavilySearchResults(k=k)\n",
        "    results = tool.run(query)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "oyfT5XoXThBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_context(doc_chunks, web_results):\n",
        "    context = \"\"\n",
        "\n",
        "    for d in doc_chunks:\n",
        "        context += f\"[Doc] {d.metadata['title']} (Chunk {d.metadata['chunk_index']}):\\n{d.page_content}\\n\\n\"\n",
        "\n",
        "    for w in web_results:\n",
        "        context += f\"[Web] {w['title']}:\\n{w['content']}\\n\\n\"\n",
        "\n",
        "    return context[:4000]\n"
      ],
      "metadata": {
        "id": "YhVG42z8Tj66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"context\"],\n",
        "        template=\"\"\"\n",
        "Answer the question using ONLY the context.\n",
        "Cite sources clearly.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    return llm(prompt.format(query=query, context=context))"
      ],
      "metadata": {
        "id": "IeWxc_JBTu4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_documents(docs):\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "    summaries = []\n",
        "    for d in docs:\n",
        "        summary = llm(f\"Summarize:\\n{d.page_content[:1000]}\")\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n"
      ],
      "metadata": {
        "id": "ZKqWVOB0Ty3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit langchain-community langchain-openai langchain-core\n",
        "import streamlit as st\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List\n",
        "import uuid\n",
        "\n",
        "# --- BEGIN: Consolidated Function Definitions and Imports ---\n",
        "\n",
        "# From cell bqB4HZ88SL9L (Document and DocumentChunk dataclasses)\n",
        "@dataclass\n",
        "class Document:\n",
        "    source_id: str\n",
        "    source_type: str\n",
        "    title: str\n",
        "    content: str\n",
        "    metadata: Dict\n",
        "\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    chunk_id: str\n",
        "    source_id: str\n",
        "    source_type: str\n",
        "    title: str\n",
        "    content: str\n",
        "    metadata: Dict\n",
        "\n",
        "@dataclass\n",
        "class WebSearchResult:\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "\n",
        "# From cell Bs9xWNyiTeX4 (classify_query)\n",
        "def classify_query(query: str):\n",
        "    query = query.lower()\n",
        "\n",
        "    if any(word in query for word in [\"latest\", \"recent\", \"current\", \"today\", \"news\"]):\n",
        "        return \"web\"\n",
        "\n",
        "    if any(word in query for word in [\"compare\", \"difference\", \"vs\"]):\n",
        "        return \"hybrid\"\n",
        "\n",
        "    return \"document\"\n",
        "\n",
        "# From cell S7V9QkdUSztO (load_faiss_index and dependencies)\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document as LCDocument\n",
        "\n",
        "def load_faiss_index(index_path=\"faiss_index\"):\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    # Ensure the index directory exists before trying to load\n",
        "    if not os.path.exists(index_path):\n",
        "        st.error(f\"FAISS index not found at '{index_path}'. Please run the indexing steps first.\")\n",
        "        return None\n",
        "    return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# From cell oyfT5XoXThBZ (tavily_search)\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "def tavily_search(query, k=5):\n",
        "    tool = TavilySearchResults(k=k)\n",
        "    results = tool.run(query)\n",
        "    # Parse TavilySearchResults output into WebSearchResult dataclass list\n",
        "    parsed_results = []\n",
        "    for res in eval(results): # TavilySearchResults.run returns a string representation of a list of dicts\n",
        "        parsed_results.append(WebSearchResult(\n",
        "            title=res.get('title', 'No Title'),\n",
        "            snippet=res.get('content', 'No Content'),\n",
        "            url=res.get('url', 'No URL')\n",
        "        ))\n",
        "    return parsed_results\n",
        "\n",
        "# From cell YhVG42z8Tj66 (build_context)\n",
        "def build_context(doc_chunks, web_results):\n",
        "    context = \"\"\n",
        "\n",
        "    for d in doc_chunks:\n",
        "        context += f\"[Doc] {d.metadata['title']} (Chunk {d.metadata['chunk_index']}):\\n{d.page_content}\\n\\n\"\n",
        "\n",
        "    for w in web_results:\n",
        "        context += f\"[Web] {w.title}:\\n{w.snippet}\\n\\n\" # Use w.snippet instead of w['content']\n",
        "\n",
        "    return context[:4000]\n",
        "\n",
        "# From cell IeWxc_JBTu4F (generate_answer and dependencies)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"context\"],\n",
        "        template=\"\"\"\n",
        "Answer the question using ONLY the context.\n",
        "Cite sources clearly.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    return llm.invoke(prompt.format(query=query, context=context))\n",
        "\n",
        "# --- END: Consolidated Function Definitions and Imports ---\n",
        "\n",
        "st.set_page_config(page_title=\"Hybrid RAG Engine\")\n",
        "\n",
        "st.sidebar.title(\" Document Manager\")\n",
        "use_web = st.sidebar.checkbox(\"Enable Web Search\", True)\n",
        "\n",
        "query = st.text_input(\"Ask your question\")\n",
        "\n",
        "if query:\n",
        "    route = classify_query(query)\n",
        "\n",
        "    db = load_faiss_index()\n",
        "    if db is None:\n",
        "        st.error(\"Cannot proceed without a FAISS index.\")\n",
        "        st.stop() # Stop execution if index is not loaded\n",
        "\n",
        "    docs, web = [], []\n",
        "\n",
        "    if route in [\"document\", \"hybrid\"]:\n",
        "        # Ensure db.similarity_search returns Document objects that have page_content\n",
        "        docs = db.similarity_search(query, k=4)\n",
        "\n",
        "    if route in [\"web\", \"hybrid\"] and use_web:\n",
        "        # tavily_search now returns list of WebSearchResult objects\n",
        "        web = tavily_search(query)\n",
        "\n",
        "    context = build_context(docs, web)\n",
        "    # generate_answer now returns an AIMessage object, extract content\n",
        "    answer_message = generate_answer(query, context)\n",
        "    answer_content = answer_message.content if hasattr(answer_message, 'content') else str(answer_message)\n",
        "\n",
        "    icon = \"üìÑ\" if route == \"document\" else \"üåê\" if route == \"web\" else \"üìä\" if route == \"hybrid\" else \"‚ùì\"\n",
        "    st.markdown(f\"### {icon} Answer\")\n",
        "    st.write(answer_content)\n",
        "\n",
        "    with st.expander(\" Document Evidence\"):\n",
        "        if docs:\n",
        "            for d in docs:\n",
        "                st.write(f\"**Title:** {d.metadata.get('title', 'N/A')} (Chunk {d.metadata.get('chunk_index', 'N/A')})\")\n",
        "                st.write(d.page_content[:300] + \"...\")\n",
        "        else:\n",
        "            st.write(\"No document evidence found.\")\n",
        "\n",
        "    with st.expander(\"Web Evidence\"):\n",
        "        if web:\n",
        "            for w in web:\n",
        "                st.write(f\"**Title:** [{w.title}]({w.url})\")\n",
        "                st.write(w.snippet[:300] + \"...\")\n",
        "        else:\n",
        "            st.write(\"No web evidence found.\")"
      ],
      "metadata": {
        "id": "mizMX00kUIwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q"
      ],
      "metadata": {
        "id": "aFLIahymUMH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "cfDYlmL2Y5pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"Hybrid RAG\", layout=\"wide\")\n",
        "\n",
        "st.title(\"Hybrid RAG Search Engine\")\n",
        "st.write(\"Streamlit is running successfully \")\n",
        "\n",
        "query = st.text_input(\"Ask something\")\n",
        "if query:\n",
        "    st.write(\"You asked:\", query)\n"
      ],
      "metadata": {
        "id": "bjqWEeSKhMnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9zKQ_yVb5w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQJYpxIuakFF",
        "outputId": "b93a2aaa-502d-48f6-f3b7-f261486b3aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20G"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "z9JhWB9oZ3n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cllrk4pJaBtm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}